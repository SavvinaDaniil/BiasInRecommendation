{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdmWUJCl6USJ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SavvinaDaniil/BiasInRecommendation/blob/main/Book%20recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZEwzP5d6USR"
      },
      "source": [
        "This notebook should be run on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCeSb4Vu6USS"
      },
      "source": [
        "# Process\n",
        "In this notebook, I will train the book recommendation algorithms using two different packages: <a href=\"http://surpriselib.com/\">Surprise</a> & <a href=\"https://cornac.readthedocs.io/en/latest/\">Cornac</a>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7QlcgcE6USU"
      },
      "source": [
        "## A. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPmwTeS66USV",
        "outputId": "edb39e7f-bdce-4e0f-de27-1bb5475f797f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cornac\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from cornac) (1.4.1)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from cornac) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cornac) (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac) (3.2.2)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac) (1.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->powerlaw->cornac) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->powerlaw->cornac) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->powerlaw->cornac) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->powerlaw->cornac) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->powerlaw->cornac) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->powerlaw->cornac) (1.15.0)\n",
            "Installing collected packages: powerlaw, cornac\n",
            "Successfully installed cornac-1.14.2 powerlaw-1.5\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise->surprise) (1.15.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1630175 sha256=fbe7eb77f6cc77ee73e568d9f0640f633cc87377bb63bcca9b9f7d5e9a2cdf5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.1 surprise-0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install cornac\n",
        "!pip install surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "47V5DR7s6USa"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random as rd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from run_algorithms import train_algorithms, train_algorithms_kf, prepare_dataset, prepare_dataset_kf\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "pd.set_option(\"display.precision\", 5)\n",
        "import cornac\n",
        "from cornac.eval_methods import RatioSplit\n",
        "from cornac.data import Reader\n",
        "from cornac.models import MostPop, MF, PMF, BPR, NeuMF, WMF, HPF, VAECF, NMF\n",
        "from cornac.metrics import MAE, MSE, RMSE, Precision, Recall, NDCG, AUC, MAP, FMeasure, MRR\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import defaultdict\n",
        "from scipy import stats\n",
        "from numpy.linalg import norm\n",
        "import seaborn as sns\n",
        "# set plot style: grey grid in the background:\n",
        "sns.set(style=\"darkgrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sxc749MS6USc"
      },
      "source": [
        "## B. Set hyperparameters\n",
        "There are certain hyperparameters that need to be tuned before the run. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KpDdroKt6USd"
      },
      "outputs": [],
      "source": [
        "item_threshold = 5 # remove users with less than item_threshold items\n",
        "user_threshold = 5 # remove items with less than user_threshold users\n",
        "top_threshold = 200 # remove users who have rated more than top_threshold items\n",
        "recommendation_type = \"books\" # books, music or movies\n",
        "item_col = \"book\" # the item column\n",
        "my_seed = 0 # random_seed\n",
        "top_fraction_items = 0.2 # the limit for an item to be considered popular\n",
        "top_fraction_users = 0.2# the limit for a user to be considered High Mainstriminess\n",
        "split_by = \"pop_fraq\" # sort users by fraction of popular items (pop_fraq) or by average popularity in profile (pop_item_fraq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HEPVX-rZ6USe"
      },
      "outputs": [],
      "source": [
        "test_size = 0.2\n",
        "rating_threshold = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSnYGMbb6USg"
      },
      "source": [
        "These additions will be useful so we can load and save the different files (plots and processed data) with clarity on the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oYgsM9m86USh"
      },
      "outputs": [],
      "source": [
        "addition_1 = \"_u\"+str(item_threshold)+\"_i\"+str(user_threshold)+\"_t\"+str(top_threshold)\n",
        "addition_2 = addition_1 + \"_tfi\"+str(int(100*top_fraction_items))\n",
        "addition_3 = addition_2 + \"_tfu\"+str(int(100*top_fraction_users))\n",
        "addition_4 = addition_3 + (\"_sbpf\" if (split_by==\"pop_fraq\") else \"_sbpif\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hmz2_4wS6USi"
      },
      "outputs": [],
      "source": [
        "rd.seed(my_seed) #seed for random functions\n",
        "np.random.seed(my_seed) #seed for all numpy fuctions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbqK-CH56USj"
      },
      "source": [
        "## C. Read files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "utMWqIUK6USk"
      },
      "outputs": [],
      "source": [
        "user_events_file = \"https://raw.githubusercontent.com/SavvinaDaniil/BiasInRecommendation/main/data/processed/\"+recommendation_type+\"/ratings\"+addition_1+\".csv\"\n",
        "high_user_file = \"https://raw.githubusercontent.com/SavvinaDaniil/BiasInRecommendation/main/data/processed/\"+recommendation_type+\"/high_users\"+addition_4+\".csv\"\n",
        "low_user_file = \"https://raw.githubusercontent.com/SavvinaDaniil/BiasInRecommendation/main/data/processed/\"+recommendation_type+\"/low_users\"+addition_4+\".csv\"\n",
        "medium_user_file = \"https://raw.githubusercontent.com/SavvinaDaniil/BiasInRecommendation/main/data/processed/\"+recommendation_type+\"/med_users\"+addition_4+\".csv\"\n",
        "df_item_dist_file = \"https://raw.githubusercontent.com/SavvinaDaniil/BiasInRecommendation/main/data/processed/\"+recommendation_type+\"/item_pop_dist\"+addition_1+\".csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ6FsCRR6USl",
        "outputId": "c0557fad-33ce-492b-93ca-778e93d34cb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "os.system(\"wget \"+user_events_file)\n",
        "os.system(\"wget \"+low_user_file)\n",
        "os.system(\"wget \"+high_user_file)\n",
        "os.system(\"wget \"+medium_user_file)\n",
        "os.system(\"wget \"+df_item_dist_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low = pd.read_csv(low_user_file, index_col=0)\n",
        "med = pd.read_csv(medium_user_file, index_col=0)\n",
        "high = pd.read_csv(high_user_file, index_col=0)"
      ],
      "metadata": {
        "id": "WsWUflDo8K25"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_item_dist = pd.read_csv(df_item_dist_file, index_col = 0)"
      ],
      "metadata": {
        "id": "pUKKcoUG81fK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH7BETV86USm"
      },
      "source": [
        "## D. Recommendation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe6or3ho6USn"
      },
      "source": [
        "### Cornac"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eMpAQsgL6USo"
      },
      "outputs": [],
      "source": [
        "# load dataset in Cornac\n",
        "reader = Reader()\n",
        "data = reader.read(user_events_file.split(\"/\")[-1],sep =\",\", skip_lines =1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lZnmi5_l6USo"
      },
      "outputs": [],
      "source": [
        "# Split the data based on ratio\n",
        "rs = RatioSplit(data=data, test_size=test_size, rating_threshold=rating_threshold, seed=my_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "otDvr-GB6USp"
      },
      "outputs": [],
      "source": [
        "# initialize models, here we are comparing: simple, traditional, and neural networks based models\n",
        "models = [\n",
        "          # 1: Random\n",
        "          # 2: MostPop\n",
        "          MostPop(),\n",
        "          # 3: UserKNN\n",
        "          # 4: BPR\n",
        "          BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),\n",
        "          # 5: MF\n",
        "          MF(k=30, max_iter=100, learning_rate=0.01, lambda_reg=0.001, seed=123),\n",
        "          # 6: PMF\n",
        "          PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001),\n",
        "          # 7: NMF\n",
        "          NMF(k=15, max_iter=50, learning_rate=0.005, lambda_u=0.06, lambda_v=0.06, lambda_bu=0.02, lambda_bi=0.02, use_bias=False, verbose=True, seed=123),\n",
        "          # 8: WMF\n",
        "          WMF(k=50, max_iter=50, learning_rate=0.001, lambda_u=0.01, lambda_v=0.01, verbose=True, seed=123),\n",
        "          # 9: PF\n",
        "          HPF(k=50, seed=123, hierarchical=False, name=\"PF\"),\n",
        "          # 10: NueMF\n",
        "          NeuMF(num_factors=8, layers=[32, 16, 8], act_fn=\"tanh\", num_epochs=1, num_neg=3, batch_size=256, lr=0.001, seed=42, verbose=True),\n",
        "          # 11: VAECF\n",
        "          VAECF(k=10, autoencoder_structure=[20], act_fn=\"tanh\", likelihood=\"mult\", n_epochs=100, batch_size=100, learning_rate=0.001, beta=1.0, seed=123, use_gpu=True, verbose=True)\n",
        "          ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erbjctPK6USq"
      },
      "outputs": [],
      "source": [
        "# define metrics to evaluate the models\n",
        "metrics = [MAE(), MSE(), RMSE(), AUC(), MAP(), MRR(), \n",
        "           Precision(k=5), Precision(k=10), Precision(k=20), Precision(k=50),\n",
        "           Recall(k=5), Recall(k=10), Recall(k=20), Recall(k=50),\n",
        "           NDCG(k=5), NDCG(k=10), NDCG(k=20), NDCG(k=50),\n",
        "           FMeasure(k=5), FMeasure(k=10), FMeasure(k=20), FMeasure(k=50)]\n",
        "\n",
        "# put it together in an experiment, voilà!\n",
        "exp = cornac.Experiment(eval_method=rs, models=models, metrics=metrics, user_based=True)\n",
        "exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_user_knn(C):\n",
        "  ctime = time.time()\n",
        "  print(\"Training User-based Collaborative Filtering...\", )\n",
        "\n",
        "  sim = C.dot(C.T)\n",
        "  norms = [norm(C[i]) for i in range(C.shape[0])]\n",
        "\n",
        "  for i in tqdm(range(C.shape[0])):\n",
        "    sim[i][i] = 0.0\n",
        "    for j in range(i+1, C.shape[0]):\n",
        "      sim[i][j] /= (norms[i] * norms[j])\n",
        "      sim[j][i] /= (norms[i] * norms[j])\n",
        "\n",
        "  print(\"Done. Elapsed time:\", time.time() - ctime, \"s\")\n",
        "  rec_score = sim.dot(C)\n",
        "  return rec_score"
      ],
      "metadata": {
        "id": "UnhS3ZT87sXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_training_data():\n",
        "  training_matrix = np.zeros((rs.train_set.matrix.shape[0], rs.train_set.matrix.shape[1]))\n",
        "  for uid in tqdm(rs.train_set.uid_map.values()):\n",
        "    for iid in rs.train_set.iid_map.values():\n",
        "      training_matrix[uid, iid] = rs.train_set.matrix[uid, iid]\n",
        "  return training_matrix"
      ],
      "metadata": {
        "id": "Hr55g2GF7xSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating users-books rating matrix (will be used for User-KNN algorithm)\n",
        "training_matrix = read_training_data()"
      ],
      "metadata": {
        "id": "V7IpeOlm70dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running User-KNN algorithms and getting the user-book scores\n",
        "user_knn_scores = compute_user_knn(training_matrix)"
      ],
      "metadata": {
        "id": "XzkJFwrJ72Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# UserKNN recommendation algorithm\n",
        "def get_top_n_UserKNN(n=10):\n",
        "    print(\"User-KNN model is selected:\")\n",
        "    top_n = defaultdict(list)\n",
        "    # test_items = list(rs.test_set.iid_map.keys())\n",
        "    for uid in rs.train_set.uid_map.values():\n",
        "      user_id = list(rs.train_set.user_ids)[uid]\n",
        "      top_n_items_idxs = list(reversed(user_knn_scores[uid].argsort()))[:n]\n",
        "      for iid in top_n_items_idxs:\n",
        "        item_id = list(rs.train_set.item_ids)[iid]\n",
        "        top_n[int(user_id)].append((int(item_id), user_knn_scores[uid][iid]))\n",
        "    return top_n"
      ],
      "metadata": {
        "id": "8R-5uOIt74me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n(algo_name, n=10):\n",
        "  for model in exp.models:\n",
        "    if model.name == algo_name:\n",
        "      print(model.name + \" model is selected:\")\n",
        "      top_n = defaultdict(list)\n",
        "      for uid in model.train_set.uid_map.values():\n",
        "        user_id = list(model.train_set.user_ids)[uid]\n",
        "        try:\n",
        "          item_rank = model.rank(user_idx=uid)[0]\n",
        "        except:\n",
        "          item_rank = model.rank(user_idx=int(uid))[0]\n",
        "        # collect top N items\n",
        "        item_rank_top = item_rank[:n]\n",
        "        for iid in item_rank_top:\n",
        "          item_id = list(model.train_set.item_ids)[iid]\n",
        "          top_n[int(user_id)].append((int(item_id), model.score(uid, iid)))\n",
        "  return top_n"
      ],
      "metadata": {
        "id": "awd4TmF476qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random recommendation algorithm\n",
        "def get_top_n_random(n=10):\n",
        "    print(\"Random model is selected:\")\n",
        "    top_n = defaultdict(list)\n",
        "    test_items = list(rs.test_set.iid_map.keys())\n",
        "    for uid in rs.train_set.uid_map.values():\n",
        "      if uid not in top_n.keys():\n",
        "        user_id = list(rs.train_set.user_ids)[uid]\n",
        "        for i in range(0, n):\n",
        "          top_n[int(user_id)].append((int(rd.choice(test_items)), i))\n",
        "    return top_n"
      ],
      "metadata": {
        "id": "zmssLS8z7883"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algo_names = ['Random', 'MostPop', 'UserKNN', 'MF', 'PMF', 'BPR', 'NMF', 'WMF', 'PF', 'NeuMF', 'VAECF']\n",
        "\n",
        "i = 0\n",
        "low_rec_gap_list = [] # one entry per algorithmus\n",
        "medium_rec_gap_list = []\n",
        "high_rec_gap_list = []\n",
        "\n",
        "for i in range(0, len(algo_names)):\n",
        "    df_item_dist[algo_names[i]] = 0\n",
        "    low_rec_gap = 0\n",
        "    medium_rec_gap = 0\n",
        "    high_rec_gap = 0\n",
        "    \n",
        "    if algo_names[i] == 'Random':\n",
        "      top_n = get_top_n_random(n=10)\n",
        "    elif algo_names[i] == 'UserKNN':\n",
        "      top_n = get_top_n_UserKNN(n=10)\n",
        "    else:\n",
        "      top_n = get_top_n(algo_names[i], n=10)\n",
        "    low_count = 0\n",
        "    med_count = 0\n",
        "    high_count = 0\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        iid_list = []\n",
        "        for (iid, _) in user_ratings:\n",
        "            df_item_dist.loc[iid, algo_names[i]] += 1\n",
        "            iid_list.append(iid)\n",
        "        gap = sum(item_dist[iid_list] / num_users) / len(iid_list)\n",
        "        if uid in low.index:\n",
        "            low_rec_gap += gap\n",
        "            low_count += 1\n",
        "        elif uid in med.index:\n",
        "            medium_rec_gap += gap\n",
        "            med_count += 1\n",
        "        elif uid in high.index:\n",
        "            high_rec_gap += gap\n",
        "            high_count += 1\n",
        "    low_rec_gap_list.append(low_rec_gap / low_count)\n",
        "    medium_rec_gap_list.append(medium_rec_gap / med_count)\n",
        "    high_rec_gap_list.append(high_rec_gap / high_count)\n",
        "    i += 1 # next algorithm"
      ],
      "metadata": {
        "id": "x3a0gJqr7-4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Book recommendation.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}